{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the process name to be human readable in htop\n",
    "import setproctitle\n",
    "setproctitle.setproctitle(\"Fetch_Top_Docks\")\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "import numpy as np\n",
    "import helper as my\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mjm_tools import zip_res_range, unzip_res_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paramaters for file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for whole project\n",
    "base_dir = \"/home/sdw95/3D_SARS2\"\n",
    "\n",
    "# Original HADDOCK Docking Output Location\n",
    "orig_dir = \"{0}/COVID19_Docking/Haddock_Output\".format(base_dir)\n",
    "\n",
    "# Destination directory to store top-ranked docked conformations\n",
    "docked_dir = \"{0}/Data/Docked_Structures\".format(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Top Docked Poses from HADDOCK Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for docked structures\n",
    "if(not os.path.exists(docked_dir)):\n",
    "    os.mkdir(docked_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    }
   ],
   "source": [
    "# Read in final output file summary for all docking attempts\n",
    "docking_trials = glob.glob(\"{0}/*/run1/structures/it1/water/file.list\".format(orig_dir))\n",
    "print len(docking_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare against list of all interactions for which docking was attempted\n",
    "attempted = glob.glob(\"{0}/*\".format(orig_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "283\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_Q99567\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_Q9BVL2\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_P61962\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_Q7Z3B4\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_Q8N0X7\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19orf8_P00750\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_Q9NZL9\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_Q15056\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_P13984\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_Q8TD19\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_Q96F45\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_P37198\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_P35658\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_Q86YT6\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19orf8_Q8N766\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_P35556\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_P35555\n",
      "/home/sdw95/3D_SARS2/COVID19_Docking/Haddock_Output/COVID19nsp9_Q9UBX5\n"
     ]
    }
   ],
   "source": [
    "# NSP9 seems to fail during topology generation\n",
    "# And two orf8 attempts did not run\n",
    "s1 = set([\"/\".join(x.split(\"/\")[:-5]) for x in docking_trials])\n",
    "s2 = set(attempted)\n",
    "print len(s1)\n",
    "print len(s2)\n",
    "for x in s2.difference(s1):\n",
    "    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03658842fc5441ff9b830ee99cad0d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=265), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Select top-ranked structure from each finished docking run\n",
    "summary = []\n",
    "best_structures = dict()\n",
    "for f in tqdm_notebook(docking_trials):\n",
    "    # (Low quality homology models retroactively removed)\n",
    "    if(\"nsp2\" in f or \"nsp4\" in f or \"orf6\" in f or \"orf9c\" in f):\n",
    "        #print \"Skipping\", f\n",
    "        continue\n",
    "    \n",
    "    p1 = f.split(\"/\")[-6].split(\"_\")[0]\n",
    "    p2 = f.split(\"/\")[-6].split(\"_\")[1]\n",
    "    \n",
    "    # Identify the top-ranked file from the docking trial summary file\n",
    "    # This file contains list of ranked docking outputs + scores...\n",
    "    #\n",
    "    # \"PREVIT:COVID19nsp8_O00566_1w.pdb\"  { 28.8738 }\n",
    "    # \"PREVIT:COVID19nsp8_O00566_18w.pdb\"  { 28.9561 }\n",
    "    for i, l in enumerate(open(f).readlines()):\n",
    "        name = l.split(\":\")[1].split(\"\\\"\")[0]\n",
    "        score = float(l.split(\":\")[1].split(\"{\")[1].split(\"}\")[0].strip())\n",
    "        \n",
    "        if(i == 0):\n",
    "            best_structures[f.split(\"/\")[-6]] = os.path.dirname(f) + \"/\" + name\n",
    "        \n",
    "        attempt = int(name.split(\"_\")[-1].split(\"w\")[0])\n",
    "        summary.append([p1, p2, attempt, score, os.path.abspath(os.path.dirname(f) + \"/\" + name), i+1])\n",
    "        \n",
    "summary = pd.DataFrame(summary, columns=[\"P1\", \"P2\", \"Attempt\", \"Score\", \"File\", \"Rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d4b76d0346469896b7362903b74bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=265), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name2score = dict()\n",
    "for f in tqdm_notebook(docking_trials):\n",
    "    # (Low quality homology models retroactively removed)\n",
    "    if(\"nsp2\" in f or \"nsp4\" in f or \"orf6\" in f or \"orf9c\" in f):\n",
    "        #print \"Skipping\", f\n",
    "        continue\n",
    "    \n",
    "    p1 = f.split(\"/\")[-6].split(\"_\")[0]\n",
    "    p2 = f.split(\"/\")[-6].split(\"_\")[1]\n",
    "    \n",
    "    # Identify the top-ranked file from the docking trial summary file\n",
    "    # This file contains list of ranked docking outputs + scores...\n",
    "    #\n",
    "    # \"PREVIT:COVID19nsp8_O00566_1w.pdb\"  { 28.8738 }\n",
    "    # \"PREVIT:COVID19nsp8_O00566_18w.pdb\"  { 28.9561 }\n",
    "    for i, l in enumerate(open(f).readlines()):\n",
    "        name = l.split(\":\")[1].split(\"\\\"\")[0]\n",
    "        score = float(l.split(\":\")[1].split(\"{\")[1].split(\"}\")[0].strip())\n",
    "        \n",
    "        attempt = int(name.split(\"_\")[-1].split(\"w\")[0])\n",
    "        \n",
    "        name2score[(p1, p2, attempt)] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy files to output directory\n",
    "for k, v in best_structures.iteritems():\n",
    "    os.system(\"cp {0} {1}/{2}_top_dock.pdb\".format(v, docked_dir, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b058972ad0d488d85ecca08938fb16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=44200), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Interface Residues\n",
    "pbar = tqdm_notebook(total=len(summary))\n",
    "def calc_ires(f, c1=\"A\", c2=\"B\"):\n",
    "    try:\n",
    "        pbar.update()\n",
    "        ires1, ires2 = my.call(\"python /home/resources/mjm_tools/irescalc.py {0} -c1 {1} -c2 {2}\".format(f, \"A\", \"B\"))\n",
    "\n",
    "        return ires1, ires2\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "# FUNCTION END\n",
    "tmp = summary[\"File\"].map(calc_ires)\n",
    "summary[\"P1_Ires\"] = [x[0] for x in tmp]\n",
    "summary[\"P2_Ires\"] = [x[1] for x in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Comparison Against ECLIAR Ires\n",
    "ires_summary = pd.read_csv(\"{0}/Data/Interface_Summary.txt\".format(base_dir), sep=\"\\t\")\n",
    "ires_summary = ires_summary[ires_summary[\"Source\"] == \"ECLAIR\"]\n",
    "interaction2ires = ires_summary.set_index([\"P1\", \"P2\"])[[\"P1_Ires\", \"P2_Ires\"]].to_dict(orient=\"index\")\n",
    "\n",
    "# Calculates interface similarity with ECLAIR results\n",
    "# and calculates the fraciton of ECLAIR predicted\n",
    "# interfaces retained in the docked interface\n",
    "def calc_stats(args):\n",
    "    #print len(args)\n",
    "    #print args\n",
    "    p1, p2, ires1, ires2 = args\n",
    "\n",
    "    # Format Sets\n",
    "    if(pd.isnull(ires1)):\n",
    "        ires1 = set()\n",
    "    else:\n",
    "        ires1 = set(ires1.split(\",\"))\n",
    "    if(pd.isnull(ires2)):\n",
    "        ires2 = set()\n",
    "    else:\n",
    "        ires2 = set(ires2.split(\",\"))\n",
    "\n",
    "    # Fetch Eclair Ires / Format Sets\n",
    "    real_ires1 = interaction2ires[(p1, p2)][\"P1_Ires\"]\n",
    "    real_ires2 = interaction2ires[(p1, p2)][\"P2_Ires\"]\n",
    "\n",
    "    if(pd.isnull(real_ires1)):\n",
    "        real_ires1 = set()\n",
    "    else:\n",
    "        real_ires1 = set(real_ires1.split(\",\"))\n",
    "    if(pd.isnull(real_ires2)):\n",
    "        real_ires2 = set()\n",
    "    else:\n",
    "        real_ires2 = set(real_ires2.split(\",\"))\n",
    "\n",
    "    # Calculate Jaccard Similarity\n",
    "    j1 = np.nan\n",
    "    try:\n",
    "        j1 = len(ires1.intersection(real_ires1)) / float(len(ires1.union(real_ires1)))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "\n",
    "    j2 = np.nan\n",
    "    try:\n",
    "        j2 = len(ires2.intersection(real_ires2)) / float(len(ires2.union(real_ires2)))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "\n",
    "    # Calculate Recall\n",
    "    r1 = np.nan\n",
    "    try:\n",
    "        r1 = len(ires1.intersection(real_ires1)) / float(len(real_ires1))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "\n",
    "    r2 = np.nan\n",
    "    try:\n",
    "        r2 = len(ires2.intersection(real_ires2)) / float(len(real_ires2))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "\n",
    "\n",
    "    return j1, j2, r1, r2\n",
    "# FUNCTION END\n",
    "tmp = summary[[\"P1\", \"P2\", \"P1_Ires\", \"P2_Ires\"]].apply(calc_stats, axis=1)\n",
    "summary[\"P1_Jaccard\"] = [x[0] for x in tmp]\n",
    "summary[\"P2_Jaccard\"] = [x[1] for x in tmp]\n",
    "summary[\"P1_Recall\"] = [x[2] for x in tmp]\n",
    "summary[\"P2_Recall\"] = [x[3] for x in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Summary (Raw version with all docks)\n",
    "# NOTE: Separate Raw / Filtered versions are theoretically\n",
    "#       only necessary here because coverage thresholds were\n",
    "#       applied after initial docking using all available\n",
    "#       structures. (i.e. in the future this should be done\n",
    "#       earlier)\n",
    "summary[[\"P1\", \"P2\", \"Attempt\", \"File\", \"Rank\", \"Score\", \"P1_Jaccard\", \"P1_Recall\", \"P1_Ires\", \"P2_Jaccard\", \"P2_Recall\", \"P2_Ires\"]].to_csv(\"{0}/Data/Docking_Summary_Raw.txt\".format(base_dir), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Docking Source to Interface Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332\n",
      "332\n"
     ]
    }
   ],
   "source": [
    "# Read Docking Summary\n",
    "docking_summary = pd.read_csv(\"{0}/Data/Docking_Summary.txt\".format(base_dir), sep=\"\\t\")\n",
    "\n",
    "# Read Interface Summary (remove any docking entries in case already been added)\n",
    "interface_summary = pd.read_csv(\"{0}/Data/Interface_Summary.txt\".format(base_dir), sep=\"\\t\")\n",
    "print len(interface_summary)\n",
    "interface_summary = interface_summary[~(interface_summary[\"Source\"] == \"Docking\")]\n",
    "print len(interface_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the entries from the top-ranked dock\n",
    "docking_summary = docking_summary[docking_summary[\"Rank\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>Attempt</th>\n",
       "      <th>File</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Score</th>\n",
       "      <th>P1_Jaccard</th>\n",
       "      <th>P1_Recall</th>\n",
       "      <th>P1_Ires</th>\n",
       "      <th>P2_Jaccard</th>\n",
       "      <th>P2_Recall</th>\n",
       "      <th>P2_Ires</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [P1, P2, Attempt, File, Rank, Score, P1_Jaccard, P1_Recall, P1_Ires, P2_Jaccard, P2_Recall, P2_Ires]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove \"Unmapped\" Residues From SIFTS Mapping (reported as negative values)\n",
    "docking_summary[\"P2_Ires\"] = docking_summary[\"P2_Ires\"].map(lambda x: \",\".join([a for a in x.split(\",\") if not \"-\" in a]))\n",
    "\n",
    "# Check for Proteins that used \"Alternate\" column in original Human PDB\n",
    "# I only know of one of these and have created a map to manually correct it\n",
    "docking_summary[docking_summary[\"P2_Ires\"].map(lambda x: not all([str.isdigit(a) for a in x.split(\",\")]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep Docking Summary data in Interface Summary Format\n",
    "id2seq = pd.read_csv(\"{0}/Data/Proteins.txt\".format(base_dir), sep=\"\\t\").set_index(\"ID\")[\"Sequence\"].to_dict()\n",
    "tmp = []\n",
    "for p1, p2, ires1, ires2 in docking_summary[[\"P1\", \"P2\", \"P1_Ires\", \"P2_Ires\"]].values:\n",
    "    tmp.append([p1, p2, \"Docking\", len(id2seq[p1]), len(ires1.split(\",\")), ires1, len(id2seq[p2]), len(ires2.split(\",\")), ires2])\n",
    "tmp = pd.DataFrame(tmp, columns=list(interface_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate results / save new summary\n",
    "interface_summary = pd.concat([interface_summary, tmp]).sort_values([\"P1\", \"P2\", \"Source\"], ascending=True)\n",
    "interface_summary.to_csv(\"{0}/Data/Interface_Summary.txt\".format(base_dir), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Docking Resuts Based on Structural Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This filtering step should really be done before docking\n",
    "#       to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only report docking results from structures where either...\n",
    "# 1. Available structure covers at least 33% of the protein\n",
    "# 2. A high-confidence ECLAIR prediction in the structure could\n",
    "#    be used as a guide during docking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This partially relies on the assumption that each human protein\n",
    "#          only interacts with one viral protein (current data) and therefore\n",
    "#          only one model exists for each protein.\n",
    "\n",
    "# Read in models used Summary\n",
    "models = pd.read_csv(\"{0}/Data/Models.txt\".format(base_dir), sep=\"\\t\")\n",
    "\n",
    "# Read in dictionary of protein ID to Protein Sequences\n",
    "id2seq = pd.read_csv(\"{0}/Data/Proteins.txt\".format(base_dir), sep=\"\\t\").set_index(\"ID\")[\"Sequence\"].to_dict()\n",
    "\n",
    "# Add coverage Info\n",
    "models[\"Len\"] = models[\"ID\"].map(lambda x: len(id2seq[x]))\n",
    "models[\"N_Covered\"] = models[\"Resi_Covered\"].map(lambda x: len(unzip_res_range(x)))\n",
    "models[\"Coverage\"] = models[\"N_Covered\"] / models[\"Len\"]\n",
    "\n",
    "# Summarize Protein to coverage info\n",
    "# Store coverage percentage, N residues covered, and list of residues covered\n",
    "id2coverage = models.set_index(\"ID\")[[\"Coverage\", \"N_Covered\", \"Resi_Covered\"]].apply(lambda x: (x[0], x[1], set([int(x) for x in unzip_res_range(x[2])])), axis=1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in original ECLAIR preds\n",
    "preds = pd.read_csv(\"{0}/Data/Interface_Summary.txt\".format(base_dir), sep=\"\\t\")\n",
    "preds = preds[preds[\"Source\"] == \"ECLAIR\"]\n",
    "\n",
    "# Parse as dictionary\n",
    "def do(iresA, iresB):\n",
    "    if(pd.isnull(iresA)):\n",
    "        iresA = set([])\n",
    "    else:\n",
    "        iresA = set([int(x) for x in str(iresA).split(\",\")])\n",
    "    \n",
    "    if(pd.isnull(iresB)):\n",
    "        iresB = set([])\n",
    "    else:\n",
    "        iresB = set([int(x) for x in str(iresB).split(\",\")])\n",
    "    \n",
    "    return (iresA, iresB)\n",
    "# FUNCTION END\n",
    "inter2ECLAIR_preds = preds[preds[\"Source\"] == \"ECLAIR\"].set_index([\"P1\", \"P2\"])[[\"P1_Ires\", \"P2_Ires\"]].apply(lambda x: do(*x), axis=1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n",
      "True\n",
      "True\n",
      "COVID19N Q9HCE1 False\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each interaction to figure out if the docking results\n",
    "# should be used\n",
    "coverage_thresh = 0.33\n",
    "n_covered_thresh = 50\n",
    "n_ires_thresh = 1\n",
    "\n",
    "dockable_inters = set()\n",
    "for p1, p2 in inter2ECLAIR_preds.keys():\n",
    "    if(p2 == \"O00203\"):\n",
    "        print \"HERE\"\n",
    "    usable1 = False\n",
    "    usable2 = False\n",
    "    \n",
    "    # Condition 0 - Has structures\n",
    "    if(not p1 in id2coverage):\n",
    "        continue\n",
    "    if(not p2 in id2coverage):\n",
    "        continue\n",
    "    \n",
    "    # Condition 1 - Sufficient Structural Coverage\n",
    "    usable1 = usable1 or id2coverage[p1][0] >= coverage_thresh\n",
    "    usable2 = usable2 or id2coverage[p2][0] >= coverage_thresh\n",
    "    \n",
    "    if(p2 == \"O00203\"):\n",
    "        print usable2\n",
    "    \n",
    "    # Condition 2 - High Confidence Ires\n",
    "    covered_preds1 = inter2ECLAIR_preds[(p1, p2)][0].intersection(id2coverage[p1][2])\n",
    "    n_covered1 = id2coverage[p1][1]\n",
    "    usable1 = usable1 or (len(covered_preds1) >= n_ires_thresh and n_covered1 >= n_covered_thresh)\n",
    "    \n",
    "    covered_preds2 = inter2ECLAIR_preds[(p1, p2)][1].intersection(id2coverage[p2][2])\n",
    "    n_covered2 = id2coverage[p2][1]\n",
    "    usable2 = usable2 or (len(covered_preds2) >= n_ires_thresh and n_covered2 >= n_covered_thresh)\n",
    "    \n",
    "    if(p2 == \"O00203\"):\n",
    "        print usable2\n",
    "    \n",
    "    if(usable1 == False):\n",
    "        print p1, p2, usable2\n",
    "    if(usable1 and usable2):\n",
    "        dockable_inters.add((p1, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECLAIR     332\n",
      "Docking    138\n",
      "Name: Source, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter and re-save interface summary\n",
    "keep_summary = interface_summary[(interface_summary[\"Source\"] == \"ECLAIR\") | (interface_summary[[\"P1\", \"P2\"]].apply(lambda x: tuple(x) in dockable_inters, axis=1))]\n",
    "print keep_summary[\"Source\"].value_counts()\n",
    "keep_summary.to_csv(\"{0}/Data/Interface_Summary.txt\".format(base_dir), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filterd docking summary (with all docking attempts)\n",
    "summary[summary[[\"P1\", \"P2\"]].apply(lambda x: tuple(x) in dockable_inters, axis=1)][[\"P1\", \"P2\", \"Attempt\", \"File\", \"Rank\", \"Score\", \"P1_Jaccard\", \"P1_Recall\", \"P1_Ires\", \"P2_Jaccard\", \"P2_Recall\", \"P2_Ires\"]].to_csv(\"{0}/Data/Docking_Summary.txt\".format(base_dir), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save modified summary for use in website\n",
    "# Only retain top rank and reformat docked file names\n",
    "# to point to their location in the Data folder with no\n",
    "# base dir\n",
    "summary_web = summary[summary[\"Rank\"] == 1].copy()\n",
    "summary_web[\"File\"] = summary_web[[\"P1\", \"P2\"]].apply(lambda x: glob.glob(\"{0}/Data/Docked_Structures/*{1}*{2}*\".format(base_dir, x[0], x[1]))[0].replace(base_dir + \"/\", \"\"), axis=1)\n",
    "summary_web.to_csv(\"{0}/Data/Docking_Summary_Charles.txt\".format(base_dir), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
