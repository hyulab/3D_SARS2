{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the process name to be human readable in htop\n",
    "import setproctitle\n",
    "setproctitle.setproctitle(\"Select_Models\")\n",
    "\n",
    "from config import *\n",
    "from helper_functions import fasta2dict, NWSeqAlignment, alignPrint, batchUniProtAPI\n",
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "import numpy as np\n",
    "import helper as my\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from mjm_tools import zip_res_range, unzip_res_range, open_pdb\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paramaters for file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for whole project\n",
    "base_dir = \"/home/sdw95/3D_SARS2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Interactions / Predicted ECLAIR Pred Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Interactions\n",
    "interactions = pd.read_csv(\"{0}/Data/Interactions.txt\".format(base_dir), sep=\"\\t\")\n",
    "\n",
    "# List of Human Interactors\n",
    "human_unis = set(interactions[\"P2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ECLAIR Preds\n",
    "inter2preds = glob.glob(\"{0}/Data/Eclair_Predictions/*\".format(base_dir))\n",
    "inter2preds = {os.path.basename(x).split(\".\")[0]:pd.read_csv(x, sep=\"\\t\") for x in inter2preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Best Available Viral Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdw95/.conda/envs/jp/lib/python2.7/site-packages/ipykernel/__main__.py:12: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/home/sdw95/.conda/envs/jp/lib/python2.7/site-packages/ipykernel/__main__.py:18: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "# Select Homology Models for COVID Proteins\n",
    "#\n",
    "# NOTE: This is done is a fairly hacky way by fetching the info from the ECLAIR feature set\n",
    "#       Homology models were originally created and stored here...\n",
    "#\n",
    "#       /home/sdw95/Collaborators/Lab_Member_Requests/Haiyuan/2020_03_27_COVID19_3DInteractome/modelling_test/{0}.*pdb\n",
    "#\n",
    "#       Since the code / data for this project got split up in multiple locations, I fetch the models from the ECLAIR\n",
    "#       feature pipeline to be positive I'm using the same models used in the predictions\n",
    "#\n",
    "interactions[\"P1 PDB\"] = interactions[\"P1\"].map(lambda x: glob.glob(\"/home/adr66/eclair/data/modbase/models/hash/{0}.pdb\".format(x.replace(\"COVID19\", \"\")))[0] if glob.glob(\"/home/adr66/eclair/data/modbase/models/hash/{0}.pdb\".format(x.replace(\"COVID19\", \"\"))) else np.nan)\n",
    "interactions.ix[interactions[\"P1\"] == \"COVID19nsp5C145A\", \"P1 PDB\"] = glob.glob(\"/home/adr66/eclair/data/modbase/models/hash/nsp5_C145A.pdb\")[0]\n",
    "\n",
    "# Manually drop low quality homology models\n",
    "# These 4 homology models were generated originally, but later\n",
    "# decided to be removed from the analysis because the template\n",
    "# quality did not pass our thresholds\n",
    "interactions.ix[interactions[\"P1\"].map(lambda x: \"nsp2\" in x or \"nsp4\" in x or \"orf6\" in x or \"orf9c\" in x), \"P1 PDB\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Homology Models with PDB Structures where available\n",
    "# For Viral proteins, replace any of the structures with newer PDB models where available\n",
    "#\n",
    "# NOTE: Code to select PDB structures for COVID proteins not provided here\n",
    "#\n",
    "prot2pdbs = defaultdict(set)\n",
    "for f in glob.glob(\"/home/sdw95/3D_SARS2/Misc_Notebooks/2021_01_13_SARS2_PDB_Sifts_Mapping/SIFTS_Mapped_PDBs/*.pdb\"):\n",
    "    prot = os.path.basename(f).split(\"_\")[0]\n",
    "    prot2pdbs[prot].add(f)\n",
    "\n",
    "def do(p1, p2, pdb):\n",
    "    if(p1 in prot2pdbs):\n",
    "        # Special case to select best structure for N protein\n",
    "        # (This is the only one that has multiple PDB structures with unique coverages)\n",
    "        if(p1 == \"COVID19N\"):\n",
    "            pair = \"_\".join(sorted([p1, p2]))\n",
    "            preds = inter2preds[pair]\n",
    "            preds = preds[preds[\"Prot\"] == (p1 > p2)]\n",
    "            \n",
    "            w1 = sum(preds[preds[\"Pos\"].map(lambda x: 44 <= x <= 180)][\"Pred\"])\n",
    "            w2 = sum(preds[preds[\"Pos\"].map(lambda x: 251 <= x <= 364)][\"Pred\"])\n",
    "            if(w1 > w2):\n",
    "                return '/home/sdw95/3D_SARS2/Misc_Notebooks/2021_01_13_SARS2_PDB_Sifts_Mapping/SIFTS_Mapped_PDBs/COVID19N_7ACT_A.pdb'\n",
    "            else:\n",
    "                return '/home/sdw95/3D_SARS2/Misc_Notebooks/2021_01_13_SARS2_PDB_Sifts_Mapping/SIFTS_Mapped_PDBs/COVID19N_6WZQ_A.pdb'\n",
    "        return list(prot2pdbs[p1])[0]\n",
    "    else:\n",
    "        return pdb\n",
    "interactions[\"P1 PDB\"] = interactions[[\"P1\", \"P2\", \"P1 PDB\"]].apply(lambda x: do(*x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Best Available Human Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SIFTS Data (to use for mapping eclair predictions onto structure\n",
    "# for selecting structure with the best cummulative ECLAIR score)\n",
    "sifts = pd.read_csv(\"/home/resources/sifts/parsed_files/pdbresiduemapping.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Best PDB Model for Human Protein (best overlap with ECLAIR Predictions)\n",
    "def get_best_pdb(cov, uni):\n",
    "    # First Fetch ECLAIR Predictions\n",
    "    pair = \"_\".join([cov, uni])\n",
    "    preds = inter2preds[pair]\n",
    "    preds = preds[preds[\"Prot\"] == (uni > cov)].set_index(\"Pos\")[\"Pred\"].to_dict()\n",
    "    \n",
    "    # Fetch all SIFTS mappings for this protein\n",
    "    tmp = sifts[sifts[\"UniProt\"] == uni][[\"PDB\", \"Chain\", \"UniProt\", \"MappableResInPDBChainOnUniprotBasis\"]].copy()\n",
    "    \n",
    "    # To filter out PDB-like structures (difficult to work with)\n",
    "    real_pdbs = [x for x in tmp[\"PDB\"].unique() if os.path.exists(\"/home/resources/pdb/data/{0}/pdb{1}.ent.gz\".format(x.lower()[1:3], x.lower()))]\n",
    "    tmp = tmp[tmp[\"PDB\"].isin(real_pdbs)]\n",
    "    \n",
    "    # NOTE: There's a manual edit here because some code needed to be re-run\n",
    "    #       and to update the summary files, and the best PDB changed since docking\n",
    "    #\n",
    "    # Manually select PDB / Chain for one protein where the selected chain\n",
    "    # changed between the original run and the current date (need to keep models\n",
    "    # file consistent with the structures that were actually used for docking)\n",
    "    if(uni == \"P15151\"):\n",
    "        return (\"3EPC\", \"R\")\n",
    "    \n",
    "    if(len(tmp) == 0):\n",
    "        return np.nan, np.nan\n",
    "    elif(len(tmp) == 1):\n",
    "        return tmp[[\"PDB\", \"Chain\"]].values[0]\n",
    "    \n",
    "    \n",
    "    tmp[\"Len\"] = tmp[\"MappableResInPDBChainOnUniprotBasis\"].map(lambda x: len(unzip_res_range(x)))\n",
    "    tmp[\"Weighted_Len\"] = tmp[\"MappableResInPDBChainOnUniprotBasis\"].map(lambda x:sum([preds[int(x)] for x in unzip_res_range(x)]))\n",
    "    \n",
    "    \n",
    "    #tmp = tmp.sort_values([\"Weighted_Len\", \"Len\"], ascending=False)\n",
    "    \n",
    "    return tmp[[\"PDB\", \"Chain\"]].values[0]    \n",
    "# FUNCTION END\n",
    "tmp = interactions[[\"P1\", \"P2\"]].apply(lambda x: get_best_pdb(*x), axis=1)\n",
    "interactions[\"P2 PDB\"] = [x[0] for x in tmp]\n",
    "interactions[\"P2 Chain\"] = [x[1] for x in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "# Interactions with human + viral structure available\n",
    "# (No ModBase Models)\n",
    "print len(interactions[(~pd.isnull(interactions[\"P1 PDB\"]))&(~pd.isnull(interactions[\"P2 PDB\"]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdw95/.conda/envs/jp/lib/python2.7/site-packages/ipykernel/__main__.py:7: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/home/sdw95/.conda/envs/jp/lib/python2.7/site-packages/ipykernel/__main__.py:8: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "# Add in Modbase Models where no PDB structure available\n",
    "modbase = pd.read_csv(\"/home/resources/modbase/parsed_files/all_modbase_models.txt\", sep=\"\\t\")\n",
    "modbase[modbase[\"modpipe_quality_score\"] >= 1.1].drop_duplicates(\"uniprot\")\n",
    "\n",
    "uni2modbase = modbase.set_index(\"uniprot\")[\"modbase_modelID\"].map(lambda x: (\"/home/resources/modbase/data/hash/{0}.pdb\".format(x))).to_dict()\n",
    "\n",
    "interactions.ix[pd.isnull(interactions[\"P2 PDB\"]), \"P2 Chain\"] = \" \"\n",
    "interactions.ix[pd.isnull(interactions[\"P2 PDB\"]), \"P2 PDB\"] = interactions.ix[pd.isnull(interactions[\"P2 PDB\"]), \"P2\"].map(lambda x: uni2modbase[x] if x in uni2modbase else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239\n"
     ]
    }
   ],
   "source": [
    "print len(interactions[(~pd.isnull(interactions[\"P1 PDB\"]))&(~pd.isnull(interactions[\"P2 PDB\"]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Local Copy of all Undocked Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3f3b44256a48858a3df50c6c23c295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# COVID protein structures were previous re-indexed so that PDB position\n",
    "# corresponds with sequence position (code not shown?)\n",
    "\n",
    "# Here we can just copy them locally\n",
    "f2f = dict()\n",
    "for f in tqdm_notebook(interactions[\"P1 PDB\"].unique()):\n",
    "    if(pd.isnull(f)):\n",
    "        f2f[f] = f\n",
    "        continue\n",
    "    \n",
    "    # Handle renaming for Homology Models Differently\n",
    "    if(\"adr66\" in f):\n",
    "        # Reassign Chain ID to \"A\" (Viral protein structure will consistently be chain A)\n",
    "        # Raw Homology Models are assigned no chain (\" \")\n",
    "        # Previously Mapped PDB structures were already reassigend chain \"A\" when reindexed \n",
    "        pdb_df = my.pdb2df(f)\n",
    "        pdb_df[\"Chain\"] = \"A\"\n",
    "        \n",
    "        # Save\n",
    "        my.df2pdb(\"{0}/Data/Undocked_Structures/COVID19{1}\".format(base_dir, os.path.basename(f)), pdb_df)\n",
    "        f2f[f] = \"{0}/Data/Undocked_Structures/COVID19{1}\".format(base_dir, os.path.basename(f))\n",
    "    else:\n",
    "        # Copy\n",
    "        os.system(\"cp {0} {1}/Data/Undocked_Structures/{2}\".format(f, base_dir, os.path.basename(f)))\n",
    "        f2f[f] = \"{0}/Data/Undocked_Structures/{1}\".format(base_dir, os.path.basename(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create position maps for all PDB files used for human structures\n",
    "# (PDB, Chain, UniProt) --> (PDB_Pos --> UniProt_Pos)\n",
    "keep_pdbs = set(interactions[\"P2 PDB\"].unique())\n",
    "pos_maps = sifts[sifts[\"PDB\"].isin(keep_pdbs)].set_index([\"PDB\", \"Chain\", \"UniProt\"])[[\"MappableResInPDBChainOnUniprotBasis\", \"MappableResInPDBChainOnPDBBasis\"]].apply(lambda x: dict(zip(unzip_res_range(x[1]), unzip_res_range(x[0]))), axis=1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7420d8bbf250457aabadf39deb5507b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=332), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Reindex structures so they are numbered by Uniprot pos\n",
    "for uni, pdb, chain in tqdm_notebook(interactions[[\"P2\", \"P2 PDB\", \"P2 Chain\"]].values):\n",
    "    #if(not uni == \"P15151\"):\n",
    "    #    continue\n",
    "    \n",
    "    if(pd.isnull(pdb)):\n",
    "        continue\n",
    "    \n",
    "    # Handle Modbase Structures (should already be indexed, just copy)\n",
    "    if(chain == \" \"):\n",
    "        # Reassign the Chain from no chain to chain \"B\" (B is consistently the human chain for this project)\n",
    "        pdb_df = my.pdb2df(pdb)\n",
    "        pdb_df[\"Chain\"] = \"B\"\n",
    "        \n",
    "        # Save\n",
    "        my.df2pdb(\"{0}/Data/Undocked_Structures/{1}_{2}\".format(base_dir, uni, os.path.basename(pdb)), pdb_df)\n",
    "        f2f[(pdb, chain)] = \"{0}/Data/Undocked_Structures/{1}_{2}\".format(base_dir, uni, os.path.basename(pdb))\n",
    "        continue\n",
    "    \n",
    "    pos_map = pos_maps[pdb, chain, uni]\n",
    "    \n",
    "    pdb_df = my.pdb2df(pdb)\n",
    "    pdb_df = pdb_df[pdb_df[\"Data Type\"] == \"ATOM\"]\n",
    "    pdb_df = pdb_df[pdb_df[\"Chain\"] == chain]\n",
    "    def do(x):\n",
    "        chain, pos = x\n",
    "        try:\n",
    "            return pos_map[str(pos)]\n",
    "        except:\n",
    "            return -pos\n",
    "    pdb_df[\"Residue ID\"] = pdb_df[[\"Chain\", \"Residue ID\"]].apply(do, axis=1)\n",
    "    \n",
    "    # Reassign the Chain\n",
    "    pdb_df[\"Chain\"] = \"B\"\n",
    "    \n",
    "    my.df2pdb(\"{0}/Data/Undocked_Structures/{1}_{2}_{3}.pdb\".format(base_dir, uni, pdb, chain), pdb_df)\n",
    "    f2f[(pdb, chain)] = \"{0}/Data/Undocked_Structures/{1}_{2}_{3}.pdb\".format(base_dir, uni, pdb, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map local location of undocked structure\n",
    "interactions[\"P1 PDB File\"] = interactions[\"P1 PDB\"].map(lambda x: f2f[x])\n",
    "interactions[\"P2 PDB File\"] = interactions[[\"P2 PDB\", \"P2 Chain\"]].apply(lambda x: f2f[tuple(x)] if not x[0] is np.nan else f2f[x[0]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ed2b1029c64cc3926515768cd2ccd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367c4da293344b2a987f797e5b89fcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=332), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for p1, p1_pdb, p1_f in tqdm_notebook(interactions[[\"P1\", \"P1 PDB\", \"P1 PDB File\"]].drop_duplicates().values):\n",
    "    p1_chain = None\n",
    "    if(pd.isnull(p1_pdb)):\n",
    "        source = None\n",
    "    elif(\"COVID\" in p1):\n",
    "        if(\"adr66\" in p1_pdb):\n",
    "            source = \"Modeller\"\n",
    "        else:\n",
    "            source = \"PDB\"\n",
    "            p1_chain = p1_pdb.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
    "    else:\n",
    "        raise\n",
    "    \n",
    "    pdb_id = None\n",
    "    pdb_chain = None\n",
    "    if(source == \"PDB\"):\n",
    "        pdb_id = p1_pdb.split(\"/\")[-1].split(\"_\")[-2].upper()\n",
    "        pdb_chain = p1_chain\n",
    "    \n",
    "    modbase_ID = None\n",
    "    if(source == \"ModBase\"):\n",
    "        modbase_ID = os.path.basename(p1_pdb).split(\".\")[0]\n",
    "    \n",
    "    all_resi = None\n",
    "    if(not source == None):\n",
    "        pdb_df = my.pdb2df(p1_f)\n",
    "        all_resi = zip_res_range([str(x) for x in sorted(pdb_df[(pdb_df[\"Data Type\"] == \"ATOM\")&(pdb_df[\"Residue ID\"] > 0)][\"Residue ID\"].unique())])\n",
    "    \n",
    "    if(not source == None):\n",
    "        models.append([p1, source, p1_f, pdb_id, pdb_chain, modbase_ID, all_resi])\n",
    "\n",
    "for p1, p1_pdb, p1_chain, p1_f in tqdm_notebook(interactions[[\"P2\", \"P2 PDB\", \"P2 Chain\", \"P2 PDB File\"]].drop_duplicates().values):\n",
    "    if(pd.isnull(p1_pdb)):\n",
    "        source = None\n",
    "    elif(\"COVID\" in p1):\n",
    "        source = \"Modeller\"\n",
    "    elif(len(p1_pdb) == 4):\n",
    "        source = \"PDB\"\n",
    "    else:\n",
    "        source = \"ModBase\"\n",
    "    \n",
    "    pdb_id = None\n",
    "    pdb_chain = None\n",
    "    if(source == \"PDB\"):\n",
    "        pdb_id = p1_pdb.upper()\n",
    "        pdb_chain = p1_chain\n",
    "    \n",
    "    modbase_ID = None\n",
    "    if(source == \"ModBase\"):\n",
    "        modbase_ID = os.path.basename(p1_pdb).split(\".\")[0]\n",
    "    \n",
    "    all_resi = None\n",
    "    if(not source == None):\n",
    "        pdb_df = my.pdb2df(p1_f)\n",
    "        all_resi = zip_res_range([str(x) for x in sorted(pdb_df[(pdb_df[\"Data Type\"] == \"ATOM\")&(pdb_df[\"Residue ID\"] > 0)][\"Residue ID\"].unique())])\n",
    "    \n",
    "    if(not source == None):\n",
    "        models.append([p1, source, p1_f, pdb_id, pdb_chain, modbase_ID, all_resi])\n",
    "models = pd.DataFrame(models, columns=[\"ID\", \"Source\", \"PDB_File\", \"PDB_ID\", \"PDB_Chain\", \"ModBase_ID\", \"Resi_Covered\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Protein Lengths\n",
    "proteins = pd.read_csv(\"{0}/Data/Proteins.txt\".format(base_dir), sep=\"\\t\")\n",
    "uni2seq = proteins.set_index(\"ID\")[\"Sequence\"].to_dict()\n",
    "\n",
    "# Calculate Coverage per model\n",
    "models[\"Coverage\"] = models[[\"ID\", \"Resi_Covered\"]].apply(lambda x: len(unzip_res_range(x[1])) / float(len(uni2seq[x[0]])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Selected Models\n",
    "models.to_csv(\"{0}/Data/Models.txt\".format(base_dir), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orient Structures for Docking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is technically unnecessary for current HADDOCK docking setup.\n",
    "#       Had previously used Rosetta and implemented ECLAIR predictions to\n",
    "#       guide docking by restricting the initial chain orientation for each\n",
    "#       dock with most likely interfaces pointed towards each other\n",
    "#\n",
    "#       It's still somewhat useful to clearly pair up the structures used for\n",
    "#       each docking trial, so I still do it here anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorient Structures for guided Docking\n",
    "if(not os.path.exists(\"{0}/Data/Oriented_Structures\".format(base_dir))):\n",
    "    os.mkdir(\"{0}/Data/Oriented_Structures\".format(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_rotation_matrix(theta):\n",
    "    return np.array([1,              0,              0,\n",
    "                     0,              np.cos(theta),  -np.sin(theta),\n",
    "                     0,              np.sin(theta),  np.cos(theta)\n",
    "                    ]).reshape((3, 3))\n",
    "# FUNCTION END\n",
    "\n",
    "def y_rotation_matrix(theta):\n",
    "    return np.array([np.cos(theta),  0,              np.sin(theta),\n",
    "                     0,              1,              0,\n",
    "                     -np.sin(theta), 0,              np.cos(theta)\n",
    "                    ]).reshape((3, 3))\n",
    "# FUNCTION END\n",
    "\n",
    "def z_rotation_matrix(theta):\n",
    "    return np.array([np.cos(theta),  -np.sin(theta), 0,\n",
    "                     np.sin(theta),  np.cos(theta), 0,\n",
    "                     0,               0,             1\n",
    "                    ]).reshape((3, 3))\n",
    "# FUNCTION END\n",
    "\n",
    "def rotate(xyz, theta_x, theta_y, theta_z, origin=np.array([0, 0, 0]), return_function=True):\n",
    "    rot_matx = x_rotation_matrix(theta_x)\n",
    "    rot_maty = y_rotation_matrix(theta_y)\n",
    "    rot_matz = z_rotation_matrix(theta_z)\n",
    "    \n",
    "    if(return_function):\n",
    "        return lambda x: rot_matz.dot(rot_maty.dot(rot_matx.dot(x - origin))) + origin\n",
    "    return rot_matz.dot(rot_maty.dot(rot_matx.dot(xyz - origin))) + origin\n",
    "# FUNCTION END\n",
    "\n",
    "def translate(df, offset):\n",
    "    df[[\"X\", \"Y\", \"Z\"]] = df[[\"X\", \"Y\", \"Z\"]] - offset\n",
    "# FUNCTION END\n",
    "\n",
    "def rotate_pdb(df, tx=None, ty=None, tz=None, origin=np.zeros(3), rot_func=None, angle_max=180):\n",
    "    if(origin is \"center\"):\n",
    "        origin = center_of_mass(pose, chain)\n",
    "    if(tx is None):\n",
    "        tx, ty, tz = (np.random.random(size=3) - 0.5)*angle_max*np.pi/180.0\n",
    "    \n",
    "    if(rot_func is None):\n",
    "        rot_func = rotate(origin, tx, ty, tz, origin, True)\n",
    "    \n",
    "    # Apply the rotation\n",
    "    tmp = np.concatenate(df[[\"X\", \"Y\", \"Z\"]].apply(lambda x: rot_func(np.array(x)), axis=1).to_list(), axis=0).reshape(len(df), 3)\n",
    "    df[\"X\"] = np.round(tmp[:,0], 3)\n",
    "    df[\"Y\"] = np.round(tmp[:,1], 3)\n",
    "    df[\"Z\"] = np.round(tmp[:,2], 3)\n",
    "# FUNCTION END\n",
    "\n",
    "def orient_pdb(df, refA, refB):\n",
    "    # Center\n",
    "    translate(df, refA)\n",
    "    refB = refB - refA\n",
    "    \n",
    "    # Generate First Rotation\n",
    "    tmp = refB\n",
    "    \n",
    "    d_x = (tmp[0] - 0)\n",
    "    d_z = (tmp[2] - 0)\n",
    "    if(d_x*d_z != 0):\n",
    "        sign = 1\n",
    "        theta1 = sign*np.arctan(d_z / d_x)\n",
    "        rot_mat1 = y_rotation_matrix(theta1)\n",
    "    else:\n",
    "        rot_mat1 = np.identity(3)\n",
    "    \n",
    "    # Generate Second Rotation\n",
    "    tmp = rot_mat1.dot(tmp)\n",
    "    \n",
    "    d_x = tmp[0] - 0\n",
    "    d_y = tmp[1] - 0\n",
    "    if(d_x*d_y != 0):\n",
    "        sign = -1#*[-1, 1][d_x*d_z <= 0]\n",
    "        theta2 = -sign*np.arctan(d_x / d_y)\n",
    "        rot_mat2 = z_rotation_matrix(theta2)\n",
    "    else:\n",
    "        rot_mat2 = np.identity(3)\n",
    "    \n",
    "    # Generate Third Rotation (to correct something?)\n",
    "    tmp = rot_mat2.dot(tmp)\n",
    "    \n",
    "    if(tmp[1] < 0):\n",
    "        theta3 = np.pi\n",
    "        rot_mat3 = z_rotation_matrix(theta3)\n",
    "    else:\n",
    "        rot_mat3 = np.identity(3)\n",
    "    \n",
    "    tmp = rot_mat3.dot(tmp)\n",
    "    \n",
    "    # Apply the rotation\n",
    "    rot_func = lambda x: rot_mat3.dot(rot_mat2.dot(rot_mat1.dot(np.array(x))))\n",
    "    rotate_pdb(df, rot_func=rot_func)\n",
    "# FUNCTION END\n",
    "\n",
    "def rot_matrix_from_plane(ref1, ref2, ref3):\n",
    "    # Center by reference 1\n",
    "    original_delta = ref1.copy()\n",
    "\n",
    "    # Rotate along Z axis so that ref 2 is at Y=0\n",
    "    tmp = ref2 - original_delta\n",
    "\n",
    "    d_x = tmp[0] - 0\n",
    "    d_y = tmp[1] - 0\n",
    "    if(d_x*d_y != 0):\n",
    "        sign = -1#*[-1, 1][d_x*d_z <= 0]\n",
    "        theta1 = sign*np.arctan(d_y / d_x)\n",
    "        rot_mat1 = z_rotation_matrix(theta1)\n",
    "    else:\n",
    "        rot_mat1 = np.identity(3)\n",
    "\n",
    "    # Roate along X axis so that ref 3 is at Z=0\n",
    "    tmp = rot_mat1.dot(ref3 - original_delta)\n",
    "\n",
    "    d_y = tmp[1] - 0\n",
    "    d_z = tmp[2] - 0\n",
    "    if(d_x*d_y != 0):\n",
    "        sign = -1#*[-1, 1][d_x*d_z <= 0]\n",
    "        theta2 = -sign*np.arctan(d_y / d_z)\n",
    "        rot_mat2 = x_rotation_matrix(theta2)\n",
    "    else:\n",
    "        rot_mat2 = np.identity(3)\n",
    "\n",
    "    # Return a function for this transformation\n",
    "    return lambda x: rot_mat2.dot(rot_mat1.dot(x - original_delta))\n",
    "# FUNCTION END\n",
    "\n",
    "def orient_pdb_by_linear_regression(df, fit_col=\"Pred\", up=True):\n",
    "    # Generate Linear Regression Fit to the provided Label Column\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(df[(~pd.isnull(df[fit_col]))&(df[\"Atom Name\"] == \"CA\")][[\"X\", \"Y\", \"Z\"]], df[(~pd.isnull(df[fit_col]))&(df[\"Atom Name\"] == \"CA\")][fit_col])\n",
    "    \n",
    "    # Generate rotation matrix to match plane\n",
    "    ref1 = np.array([-clf.intercept_/clf.coef_[0], 0, 0])\n",
    "    ref2 = np.array([0, -clf.intercept_/clf.coef_[1], 0])\n",
    "    ref3 = np.array([0, 0, -clf.intercept_/clf.coef_[2]])\n",
    "    \n",
    "    rot_func = rot_matrix_from_plane(ref1, ref2, ref3)\n",
    "    \n",
    "    # Apply Rotation and Center\n",
    "    rotate_pdb(df, rot_func=rot_func)\n",
    "    translate(df, df[[\"X\", \"Y\", \"Z\"]].mean().values)\n",
    "    \n",
    "    # Final rotation to make sure interface is pointing the right direction\n",
    "    if(up == True):\n",
    "        if(clf.coef_[1] < 0):\n",
    "            rotate_pdb(df, tx=np.deg2rad(180), ty=0, tz=0)\n",
    "    else:\n",
    "        if(clf.coef_[1] > 0):\n",
    "            rotate_pdb(df, tx=np.deg2rad(180), ty=0, tz=0)\n",
    "# FUNCTION END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b2487f54b44e78adfdfdb3b93c29fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=332), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Reorient all structures so that interface faces are facing each other\n",
    "for uniA, uniB, pdbA, pdbB, chainB in tqdm_notebook(interactions[[\"P1\", \"P2\", \"P1 PDB File\", \"P2 PDB File\", \"P2 Chain\"]].values):\n",
    "    if(os.path.exists(\"{0}/Data/Oriented_Structures/{1}_{2}.pdb\".format(base_dir, uniA, uniB))):\n",
    "        continue\n",
    "    \n",
    "    chainA = \" \"\n",
    "    \n",
    "    if(pd.isnull(pdbA) or pd.isnull(pdbB)):\n",
    "        continue\n",
    "    \n",
    "    # Read Structures\n",
    "    A_df = my.pdb2df(pdbA)\n",
    "    if(not chainA == \" \"):\n",
    "        A_df = A_df[A_df[\"Chain\"] == chainA].copy()\n",
    "    \n",
    "    B_df = my.pdb2df(pdbB)\n",
    "    #if(not chainB == \" \"):\n",
    "    #    B_df = B_df[B_df[\"Chain\"] == chainB].copy()\n",
    "    \n",
    "    if(len(A_df) == 0 or len(B_df) == 0):\n",
    "        print \"ERROR: no data for\", uniA, uniB, pdbA, pdbB, chainA, chainB, len(A_df), len(B_df)\n",
    "        continue\n",
    "    \n",
    "    # Get Eclair Preds\n",
    "    tmp = inter2preds[\"_\".join([uniA, uniB])]\n",
    "    tmp = tmp.set_index([\"Prot\", \"Pos\"])[\"Pred\"].to_dict()\n",
    "    prot = 0 #int(uniA > uniB)\n",
    "    \n",
    "    # Assign Prediction Scores\n",
    "    if(uniA != uniB):\n",
    "        A_df[\"Pred\"] = A_df[\"Residue ID\"].map(lambda x: tmp[(prot, x)] if x > 0 else np.nan)\n",
    "        B_df[\"Pred\"] = B_df[\"Residue ID\"].map(lambda x: tmp[(1-prot, x)] if x > 0 else np.nan)\n",
    "    else:\n",
    "        A_df[\"Pred\"] = A_df[\"Residue ID\"].map(lambda x: tmp[(0, x)] if x > 0 else np.nan)\n",
    "        B_df[\"Pred\"] = B_df[\"Residue ID\"].map(lambda x: tmp[(0, x)] if x > 0 else np.nan)\n",
    "    \n",
    "    \n",
    "    # Begin Reorientation\n",
    "    \n",
    "    # Handle Chain A\n",
    "    orient_pdb_by_linear_regression(A_df, up=True)\n",
    "    \n",
    "    # Handle Chain B\n",
    "    orient_pdb_by_linear_regression(B_df, up=False)\n",
    "    \n",
    "    # Final Translation to separate chains by 5 A\n",
    "    translate(A_df, np.array([0, A_df[\"Y\"].max() + 2.5, 0]))\n",
    "    translate(B_df, np.array([0, B_df[\"Y\"].min() - 2.5, 0]))\n",
    "    \n",
    "    # ADD CODE TO OPTIMZIE ROTATION ALONG Y AXIS (ONLY IN CHAIN B)\n",
    "    \n",
    "    \n",
    "    # Record Centroid Locations\n",
    "    ires_centroid_A = A_df[(A_df[\"Atom Name\"] == \"CA\")&(~pd.isnull(A_df[\"Pred\"]))][[\"X\", \"Y\", \"Z\", \"Pred\"]].apply(lambda x: np.array(x[:3])*x[3], axis=1).sum() / A_df[(A_df[\"Atom Name\"] == \"CA\")&(~pd.isnull(A_df[\"Pred\"]))][\"Pred\"].sum()\n",
    "    ires_centroid_B = B_df[(B_df[\"Atom Name\"] == \"CA\")&(~pd.isnull(B_df[\"Pred\"]))][[\"X\", \"Y\", \"Z\", \"Pred\"]].apply(lambda x: np.array(x[:3])*x[3], axis=1).sum() / B_df[(B_df[\"Atom Name\"] == \"CA\")&(~pd.isnull(B_df[\"Pred\"]))][\"Pred\"].sum()\n",
    "    \n",
    "    # Make sure X:Z centorids of the Interfaces Line Up\n",
    "    translate(A_df, np.array([ires_centroid_A[0], 0, ires_centroid_A[2]]))\n",
    "    translate(B_df, np.array([ires_centroid_B[0], 0, ires_centroid_B[2]]))\n",
    "    \n",
    "    # Save Reoriented Structures\n",
    "    A_df[\"Chain\"] = \"A\"\n",
    "    B_df[\"Chain\"] = \"B\"\n",
    "    my.df2pdb(\"{0}/Data/Oriented_Structures/{1}_{2}.pdb\".format(base_dir, uniA, uniB), pd.concat([A_df, B_df]))\n",
    "    my.df2pdb(\"{0}/Data/Oriented_Structures/{1}_{2}_A.pdb\".format(base_dir, uniA, uniB), A_df)\n",
    "    my.df2pdb(\"{0}/Data/Oriented_Structures/{1}_{2}_B.pdb\".format(base_dir, uniA, uniB), B_df)\n",
    "    \n",
    "    \n",
    "    ## Create Pymol Session\n",
    "    #pymolCmd.reinitialize()\n",
    "    #\n",
    "    ## Load Original Structure\n",
    "    #name = \"{0}_{1}\".format(uniA, uniB)\n",
    "    #\n",
    "    ## Load Reoriented Structures\n",
    "    #pymolCmd.load(\"oriented_structures/{0}_A.pdb\".format(name), name + \"_A\")\n",
    "    #pymolCmd.load(\"oriented_structures/{0}_B.pdb\".format(name), name + \"_B\")\n",
    "    #\n",
    "    #\n",
    "    ## Color By Prediction\n",
    "    #cmapA = cm.get_cmap('Greens')   \n",
    "    #cmapB = cm.get_cmap('Blues')\n",
    "    #\n",
    "    #for resi, pred in A_df[[\"Residue ID\", \"Pred\"]].drop_duplicates().values:\n",
    "    #    if(np.isnan(pred)):\n",
    "    #        pred = 0\n",
    "    #    pymolCmd.color(matplotlib.colors.rgb2hex(cmapA(float(pred))[:3]).replace(\"#\", \"0x\"), \"{0}_A and resi {2}\".format(name, \"A\", int(resi)))\n",
    "    #    \n",
    "    #for resi, pred in B_df[[\"Residue ID\", \"Pred\"]].drop_duplicates().values:\n",
    "    #    if(np.isnan(pred)):\n",
    "    #        pred = 0\n",
    "    #    pymolCmd.color(matplotlib.colors.rgb2hex(cmapB(float(pred))[:3]).replace(\"#\", \"0x\"), \"{0}_B and resi {2}\".format(name, \"B\", int(resi)))\n",
    "    #\n",
    "    ## Save Structure Session\n",
    "    #pymolCmd.save(\"PyMolSessions/{0}.pse\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
