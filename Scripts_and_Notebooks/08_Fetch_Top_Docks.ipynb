{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the process name to be human readable in htop\n",
    "import setproctitle\n",
    "setproctitle.setproctitle(\"08_Fetch_Top_Docks\")\n",
    "\n",
    "from config import *\n",
    "from helper_functions import zip_res_range, unzip_res_range\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "import numpy as np\n",
    "import helper as my\n",
    "import subprocess as sp\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook selects tht top-scored docking output from each of the HADDOCK runs to use in downstream analysis. It additionally calculates the interface residues from these top-scored docking outputs, compares them against the original ECLAIR predictions, and produces the Docking_Summary.txt. It also updates the Interface_Summary.txt to add interface annotations from the docking results.\n",
    "\n",
    "**NOTE:** At this stage AFTER docking is already run, docked outputs are filtered to only retain for downstream analysis docked interactions whose structures had sufficient coverage, or included a high-confidence interface prediction that was used to guide docking. Practically speaking this filtering step should instead be applied before docking is run at all to save time.\n",
    "\n",
    "- Inputs:\n",
    "  - [P1]\\_[P2] (Interaction Haddock Run Directory created under \"Docking_Runs\")\n",
    "  - Interface_Summary.txt\n",
    "  - Proteins.txt\n",
    "  - Models.txt\n",
    "\n",
    "\n",
    "- Outputs:\n",
    "  - [P1]\\_[P2]_top_dock.pdb\n",
    "  - Docking_Summary\n",
    "  - Interface_Summary.txt (updated)\n",
    "\n",
    "\n",
    "- Dependencies:\n",
    "  - Must be run after 07_Run_PPI_Docking and 03_Generate_Proteins\n",
    "  - Calls irescalc.py\n",
    "    - **NOTE:** irescalc.py *may not* be currently properly extraced from the Yu Lab's server and may not run successfully in this repository. The raw code is provided, but it itself calls several separate dependencies, and I have not been able to thoroughly confirm there are no specifics to our machine still linked to it.\n",
    "    - I *believe* it should be functional, but if any end user encounters errors runngin irescalc.py from this repository please contact the authors.\n",
    "    - Requires NACCESS installed locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Top Docked Poses from HADDOCK Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for docked structures\n",
    "if(not os.path.exists(docking_dir)):\n",
    "    os.mkdir(docking_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Read in final output file summary for all docking attempts\n",
    "docking_trials = glob.glob(\"{0}/*/run1/structures/it1/water/file.list\".format(raw_docking_dir))\n",
    "print len(docking_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare against list of all interactions for which docking was attempted\n",
    "attempted = glob.glob(\"{0}/*\".format(raw_docking_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# NOTE: From my experience a small number of protein structures will fail during topology generation\n",
    "#       so its worth checking the difference between what was attempted and what actually produced\n",
    "#       a final output to troubleshoot any issues.\n",
    "s1 = set([\"/\".join(x.split(\"/\")[:-5]) for x in docking_trials])\n",
    "s2 = set(attempted)\n",
    "print len(s1)\n",
    "print len(s2)\n",
    "\n",
    "# Print any attempts with no final output\n",
    "for x in s2.difference(s1):\n",
    "    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7f0b726a6946ef928e8aa16ddbd044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select top-ranked structure from each finished docking run\n",
    "summary = []\n",
    "best_structures = dict()\n",
    "for f in tqdm_notebook(docking_trials):\n",
    "    # (Low quality homology models retroactively removed)\n",
    "    # (This manual step would need to be updated if real\n",
    "    #  PDB structures or better homology models become\n",
    "    #  available for these proteins)\n",
    "    if(\"nsp2\" in f or \"nsp4\" in f or \"orf6\" in f or \"orf9c\" in f):\n",
    "        print \"Skipping\", f, \"manually filterd homology model\"\n",
    "        continue\n",
    "    \n",
    "    p1 = f.split(\"/\")[-6].split(\"_\")[0]\n",
    "    p2 = f.split(\"/\")[-6].split(\"_\")[1]\n",
    "    \n",
    "    # Identify the top-ranked file from the docking trial summary file\n",
    "    # This file contains list of ranked docking outputs + scores...\n",
    "    #\n",
    "    # \"PREVIT:COVID19nsp8_O00566_1w.pdb\"  { 28.8738 }\n",
    "    # \"PREVIT:COVID19nsp8_O00566_18w.pdb\"  { 28.9561 }\n",
    "    for i, l in enumerate(open(f).readlines()):\n",
    "        name = l.split(\":\")[1].split(\"\\\"\")[0]\n",
    "        score = float(l.split(\":\")[1].split(\"{\")[1].split(\"}\")[0].strip())\n",
    "        \n",
    "        if(i == 0):\n",
    "            best_structures[f.split(\"/\")[-6]] = os.path.dirname(f) + \"/\" + name\n",
    "        \n",
    "        attempt = int(name.split(\"_\")[-1].split(\"w\")[0])\n",
    "        summary.append([p1, p2, attempt, score, os.path.abspath(os.path.dirname(f) + \"/\" + name), i+1])\n",
    "        \n",
    "summary = pd.DataFrame(summary, columns=[\"P1\", \"P2\", \"Attempt\", \"Score\", \"File\", \"Rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64af7a18649847a089f16c455bc3b4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name2score = dict()\n",
    "for f in tqdm_notebook(docking_trials):\n",
    "    # (Low quality homology models retroactively removed)\n",
    "    if(\"nsp2\" in f or \"nsp4\" in f or \"orf6\" in f or \"orf9c\" in f):\n",
    "        #print \"Skipping\", f\n",
    "        continue\n",
    "    \n",
    "    p1 = f.split(\"/\")[-6].split(\"_\")[0]\n",
    "    p2 = f.split(\"/\")[-6].split(\"_\")[1]\n",
    "    \n",
    "    # Identify the top-ranked file from the docking trial summary file\n",
    "    # This file contains list of ranked docking outputs + scores...\n",
    "    #\n",
    "    # \"PREVIT:COVID19nsp8_O00566_1w.pdb\"  { 28.8738 }\n",
    "    # \"PREVIT:COVID19nsp8_O00566_18w.pdb\"  { 28.9561 }\n",
    "    for i, l in enumerate(open(f).readlines()):\n",
    "        name = l.split(\":\")[1].split(\"\\\"\")[0]\n",
    "        score = float(l.split(\":\")[1].split(\"{\")[1].split(\"}\")[0].strip())\n",
    "        \n",
    "        attempt = int(name.split(\"_\")[-1].split(\"w\")[0])\n",
    "        \n",
    "        name2score[(p1, p2, attempt)] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy files to output directory\n",
    "for k, v in best_structures.iteritems():\n",
    "    os.system(\"cp {0} {1}/{2}_top_dock.pdb\".format(v, docking_dir, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2792ff0d944b0f99c79de4a628dfee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate Interface Residues\n",
    "# NOTE: By default this is done for ALL 200 docking outputs\n",
    "#       from each trial not just the top-ranked output we use\n",
    "#       for downstream analysis. This may not be necessary for\n",
    "#       all use-cases and adds significant run-time if the\n",
    "#       interfaces from all docking outputs are not desired.\n",
    "#       \n",
    "#       e.g. The provided example input would run in ~10 seconds\n",
    "#       using only the top-ranked output compared to ~30 minutes\n",
    "#       using ALL outputs.\n",
    "pbar = tqdm_notebook(total=len(summary))\n",
    "irescalc_path = \"{0}/irescalc.py\".format(script_dir)\n",
    "def calc_ires(f, c1=\"A\", c2=\"B\"):\n",
    "    try:\n",
    "        pbar.update()\n",
    "        ires1, ires2 = sp.check_output(\"{0} {1} -c1 {2} -c2 {3}\".format(irescalc_path, f, \"A\", \"B\"), shell=True).split(\"\\n\")[:2]\n",
    "\n",
    "        return ires1, ires2\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        raise\n",
    "        return np.nan, np.nan\n",
    "# FUNCTION END\n",
    "tmp = summary[\"File\"].map(calc_ires)\n",
    "summary[\"P1_Ires\"] = [x[0] for x in tmp]\n",
    "summary[\"P2_Ires\"] = [x[1] for x in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Comparison Against ECLIAR Ires\n",
    "ires_summary = pd.read_csv(\"{0}/Interface_Summary.txt\".format(output_dir), sep=\"\\t\")\n",
    "ires_summary = ires_summary[ires_summary[\"Source\"] == \"ECLAIR\"]\n",
    "interaction2ires = ires_summary.set_index([\"P1\", \"P2\"])[[\"P1_Ires\", \"P2_Ires\"]].to_dict(orient=\"index\")\n",
    "\n",
    "# Calculates interface similarity with ECLAIR results\n",
    "# and calculates the fraciton of ECLAIR predicted\n",
    "# interfaces retained in the docked interface\n",
    "def calc_stats(args):\n",
    "    #print len(args)\n",
    "    #print args\n",
    "    p1, p2, ires1, ires2 = args\n",
    "\n",
    "    # Format Sets\n",
    "    if(pd.isnull(ires1)):\n",
    "        ires1 = set()\n",
    "    else:\n",
    "        ires1 = set(ires1.split(\",\"))\n",
    "    if(pd.isnull(ires2)):\n",
    "        ires2 = set()\n",
    "    else:\n",
    "        ires2 = set(ires2.split(\",\"))\n",
    "\n",
    "    # Fetch Eclair Ires / Format Sets\n",
    "    real_ires1 = interaction2ires[(p1, p2)][\"P1_Ires\"]\n",
    "    real_ires2 = interaction2ires[(p1, p2)][\"P2_Ires\"]\n",
    "\n",
    "    if(pd.isnull(real_ires1)):\n",
    "        real_ires1 = set()\n",
    "    else:\n",
    "        real_ires1 = set(real_ires1.split(\",\"))\n",
    "    if(pd.isnull(real_ires2)):\n",
    "        real_ires2 = set()\n",
    "    else:\n",
    "        real_ires2 = set(real_ires2.split(\",\"))\n",
    "\n",
    "    # Calculate Jaccard Similarity\n",
    "    j1 = np.nan\n",
    "    try:\n",
    "        j1 = len(ires1.intersection(real_ires1)) / float(len(ires1.union(real_ires1)))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "\n",
    "    j2 = np.nan\n",
    "    try:\n",
    "        j2 = len(ires2.intersection(real_ires2)) / float(len(ires2.union(real_ires2)))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "\n",
    "    # Calculate Recall\n",
    "    r1 = np.nan\n",
    "    try:\n",
    "        r1 = len(ires1.intersection(real_ires1)) / float(len(real_ires1))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "\n",
    "    r2 = np.nan\n",
    "    try:\n",
    "        r2 = len(ires2.intersection(real_ires2)) / float(len(real_ires2))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "\n",
    "\n",
    "    return j1, j2, r1, r2\n",
    "# FUNCTION END\n",
    "tmp = summary[[\"P1\", \"P2\", \"P1_Ires\", \"P2_Ires\"]].apply(calc_stats, axis=1)\n",
    "summary[\"P1_Jaccard\"] = [x[0] for x in tmp]\n",
    "summary[\"P2_Jaccard\"] = [x[1] for x in tmp]\n",
    "summary[\"P1_Recall\"] = [x[2] for x in tmp]\n",
    "summary[\"P2_Recall\"] = [x[3] for x in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Summary (Raw version with all docks included + no coverage limits)\n",
    "# NOTE: Separate Raw / Filtered versions are theoretically\n",
    "#       only necessary here because coverage thresholds were\n",
    "#       applied after initial docking using all available\n",
    "#       structures. (i.e. in the future this coverage filter\n",
    "#       should isntead be applied suring the select models\n",
    "#       step to avoid even running the docking trials in cases\n",
    "#       where they will be filtered anyway.)\n",
    "summary[[\"P1\", \"P2\", \"Attempt\", \"File\", \"Rank\", \"Score\", \"P1_Jaccard\", \"P1_Recall\", \"P1_Ires\", \"P2_Jaccard\", \"P2_Recall\", \"P2_Ires\"]].to_csv(\"{0}/Docking_Summary_Raw.txt\".format(output_dir), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Docking Source to Interface Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Read Docking Summary\n",
    "docking_summary = pd.read_csv(\"{0}/Docking_Summary_Raw.txt\".format(output_dir), sep=\"\\t\")\n",
    "\n",
    "# Read Interface Summary (remove any docking entries in case already been added)\n",
    "interface_summary = pd.read_csv(\"{0}/Interface_Summary.txt\".format(output_dir), sep=\"\\t\")\n",
    "print len(interface_summary)\n",
    "interface_summary = interface_summary[~(interface_summary[\"Source\"] == \"Docking\")]\n",
    "print len(interface_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the entries from the top-ranked dock\n",
    "docking_summary = docking_summary[docking_summary[\"Rank\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>Attempt</th>\n",
       "      <th>File</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Score</th>\n",
       "      <th>P1_Jaccard</th>\n",
       "      <th>P1_Recall</th>\n",
       "      <th>P1_Ires</th>\n",
       "      <th>P2_Jaccard</th>\n",
       "      <th>P2_Recall</th>\n",
       "      <th>P2_Ires</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [P1, P2, Attempt, File, Rank, Score, P1_Jaccard, P1_Recall, P1_Ires, P2_Jaccard, P2_Recall, P2_Ires]\n",
       "Index: []"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove \"Unmapped\" Residues From SIFTS Mapping (reported as negative values)\n",
    "docking_summary[\"P2_Ires\"] = docking_summary[\"P2_Ires\"].map(lambda x: \",\".join([a for a in x.split(\",\") if not \"-\" in a]))\n",
    "\n",
    "# Check for Proteins that used \"Alternate\" column in original Human PDB\n",
    "# I only know of one of these and have created a map to manually correct it\n",
    "docking_summary[docking_summary[\"P2_Ires\"].map(lambda x: not all([str.isdigit(a) for a in x.split(\",\")]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep Docking Summary data in Interface Summary Format\n",
    "id2seq = pd.read_csv(\"{0}/Proteins.txt\".format(output_dir), sep=\"\\t\").set_index(\"ID\")[\"Sequence\"].to_dict()\n",
    "tmp = []\n",
    "for p1, p2, ires1, ires2 in docking_summary[[\"P1\", \"P2\", \"P1_Ires\", \"P2_Ires\"]].values:\n",
    "    tmp.append([p1, p2, \"Docking\", len(id2seq[p1]), len(ires1.split(\",\")), ires1, len(id2seq[p2]), len(ires2.split(\",\")), ires2])\n",
    "tmp = pd.DataFrame(tmp, columns=list(interface_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate results / save new summary\n",
    "interface_summary = pd.concat([interface_summary, tmp]).sort_values([\"P1\", \"P2\", \"Source\"], ascending=True)\n",
    "interface_summary.to_csv(\"{0}/Interface_Summary.txt\".format(output_dir), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Docking Resuts Based on Structural Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This filtering step should really be done before docking\n",
    "#       to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only report docking results from structures where either...\n",
    "# 1. Available structure covers at least 33% of the protein\n",
    "# 2. A high-confidence ECLAIR prediction in the structure could\n",
    "#    be used as a guide during docking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This partially relies on the assumption that each human protein\n",
    "#          only interacts with one viral protein (current data) and therefore\n",
    "#          only one model exists for each protein.\n",
    "\n",
    "# Read in models used Summary\n",
    "models = pd.read_csv(\"{0}/Models.txt\".format(output_dir), sep=\"\\t\")\n",
    "\n",
    "# Read in dictionary of protein ID to Protein Sequences\n",
    "id2seq = pd.read_csv(\"{0}/Proteins.txt\".format(output_dir), sep=\"\\t\").set_index(\"ID\")[\"Sequence\"].to_dict()\n",
    "\n",
    "# Add coverage Info\n",
    "models[\"Len\"] = models[\"ID\"].map(lambda x: len(id2seq[x]))\n",
    "models[\"N_Covered\"] = models[\"Resi_Covered\"].map(lambda x: len(unzip_res_range(x)))\n",
    "models[\"Coverage\"] = models[\"N_Covered\"] / models[\"Len\"]\n",
    "\n",
    "# Summarize Protein to coverage info\n",
    "# Store coverage percentage, N residues covered, and list of residues covered\n",
    "id2coverage = models.set_index(\"ID\")[[\"Coverage\", \"N_Covered\", \"Resi_Covered\"]].apply(lambda x: (x[0], x[1], set([int(x) for x in unzip_res_range(x[2])])), axis=1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in original ECLAIR preds\n",
    "preds = pd.read_csv(\"{0}/Interface_Summary.txt\".format(output_dir), sep=\"\\t\")\n",
    "preds = preds[preds[\"Source\"] == \"ECLAIR\"]\n",
    "\n",
    "# Parse as dictionary\n",
    "def do(iresA, iresB):\n",
    "    if(pd.isnull(iresA)):\n",
    "        iresA = set([])\n",
    "    else:\n",
    "        iresA = set([int(x) for x in str(iresA).split(\",\")])\n",
    "    \n",
    "    if(pd.isnull(iresB)):\n",
    "        iresB = set([])\n",
    "    else:\n",
    "        iresB = set([int(x) for x in str(iresB).split(\",\")])\n",
    "    \n",
    "    return (iresA, iresB)\n",
    "# FUNCTION END\n",
    "inter2ECLAIR_preds = preds[preds[\"Source\"] == \"ECLAIR\"].set_index([\"P1\", \"P2\"])[[\"P1_Ires\", \"P2_Ires\"]].apply(lambda x: do(*x), axis=1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each interaction to figure out if the docking results\n",
    "# should be used\n",
    "coverage_thresh = 0.33\n",
    "n_covered_thresh = 50\n",
    "n_ires_thresh = 1\n",
    "\n",
    "dockable_inters = set()\n",
    "for p1, p2 in inter2ECLAIR_preds.keys():\n",
    "    usable1 = False\n",
    "    usable2 = False\n",
    "    \n",
    "    # Condition 0 - Has structures\n",
    "    if(not p1 in id2coverage):\n",
    "        continue\n",
    "    if(not p2 in id2coverage):\n",
    "        continue\n",
    "    \n",
    "    # Condition 1 - Sufficient Structural Coverage\n",
    "    usable1 = usable1 or id2coverage[p1][0] >= coverage_thresh\n",
    "    usable2 = usable2 or id2coverage[p2][0] >= coverage_thresh\n",
    "    \n",
    "    # Condition 2 - High Confidence Ires\n",
    "    covered_preds1 = inter2ECLAIR_preds[(p1, p2)][0].intersection(id2coverage[p1][2])\n",
    "    n_covered1 = id2coverage[p1][1]\n",
    "    usable1 = usable1 or (len(covered_preds1) >= n_ires_thresh and n_covered1 >= n_covered_thresh)\n",
    "    \n",
    "    covered_preds2 = inter2ECLAIR_preds[(p1, p2)][1].intersection(id2coverage[p2][2])\n",
    "    n_covered2 = id2coverage[p2][1]\n",
    "    usable2 = usable2 or (len(covered_preds2) >= n_ires_thresh and n_covered2 >= n_covered_thresh)\n",
    "    \n",
    "    if(usable1 and usable2):\n",
    "        dockable_inters.add((p1, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECLAIR     10\n",
      "Docking     7\n",
      "Name: Source, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter and re-save interface summary\n",
    "keep_summary = interface_summary[(interface_summary[\"Source\"] == \"ECLAIR\") | (interface_summary[[\"P1\", \"P2\"]].apply(lambda x: tuple(x) in dockable_inters, axis=1))]\n",
    "print keep_summary[\"Source\"].value_counts()\n",
    "keep_summary.to_csv(\"{0}/Interface_Summary.txt\".format(output_dir), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filterd docking summary (with all docking attempts)\n",
    "summary[summary[[\"P1\", \"P2\"]].apply(lambda x: tuple(x) in dockable_inters, axis=1)][[\"P1\", \"P2\", \"Attempt\", \"File\", \"Rank\", \"Score\", \"P1_Jaccard\", \"P1_Recall\", \"P1_Ires\", \"P2_Jaccard\", \"P2_Recall\", \"P2_Ires\"]].to_csv(\"{0}/Docking_Summary.txt\".format(output_dir), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
